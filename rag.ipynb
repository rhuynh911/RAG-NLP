{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa50bd8",
   "metadata": {},
   "source": [
    "<H3>Install libs<H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán ch√≠nh cho m√¥ h√¨nh ng√¥n ng·ªØ, embedding, RAG v√† giao di·ªán web\n",
    "!pip install -q \\\n",
    "  \"torch>=2.0.0\" \\\n",
    "  \"transformers>=4.40.0\" \\\n",
    "  \"accelerate>=0.30.0\" \\\n",
    "  \"huggingface-hub>=0.23.0\" \\\n",
    "  \"sentence-transformers>=2.7.0\" \\\n",
    "  \"langchain>=0.2.0\" \\\n",
    "  \"langchain-core>=0.2.0\" \\\n",
    "  \"langchain-community>=0.1.0\" \\\n",
    "  \"langchain-text-splitters>=0.2.0\" \\\n",
    "  \"chromadb>=0.5.0\" \\\n",
    "  \"langchain-chroma>=0.2.0\" \\\n",
    "  \"pypdf>=4.2.0\" \\\n",
    "  \"gradio>=5.0.0\" \\\n",
    "  \"langchain-huggingface\" \\\n",
    "  \"wget\" \\\n",
    "  \"tqdm\" \\\n",
    "  \"ipywidgets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bbe7be",
   "metadata": {},
   "source": [
    "<H3>Setup project + t·∫°o c·∫•u tr√∫c th∆∞ m·ª•c</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# Root d·ª± √°n: d√πng folder \"rag_langchain\" n·∫±m c√πng c·∫•p notebook (nh∆∞ ·∫£nh b·∫°n)\n",
    "PROJECT_ROOT = os.path.abspath(\"rag_langchain\")\n",
    "\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data_source\", \"generative_ai\")  # b·∫°n copy PDF v√†o ƒë√¢y\n",
    "CUSTOM_DIR = os.path.join(PROJECT_ROOT, \"data_source\", \"custom\")       # tu·ª≥ ch·ªçn\n",
    "CHROMA_DIR = os.path.join(PROJECT_ROOT, \"chroma_data\")                 # l∆∞u vector DB\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CUSTOM_DIR, exist_ok=True)\n",
    "os.makedirs(CHROMA_DIR, exist_ok=True)\n",
    "\n",
    "# src (tu·ª≥ ch·ªçn, cho ƒë√∫ng c·∫•u tr√∫c t√†i li·ªáu)\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, \"src\", \"base\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, \"src\", \"rag\"), exist_ok=True)\n",
    "\n",
    "# t·∫°o __init__.py\n",
    "for p in [\n",
    "    os.path.join(PROJECT_ROOT, \"src\", \"__init__.py\"),\n",
    "    os.path.join(PROJECT_ROOT, \"src\", \"base\", \"__init__.py\"),\n",
    "    os.path.join(PROJECT_ROOT, \"src\", \"rag\", \"__init__.py\"),\n",
    "]:\n",
    "    if not os.path.exists(p):\n",
    "        open(p, \"w\", encoding=\"utf-8\").close()\n",
    "\n",
    "# th√™m PROJECT_ROOT v√†o sys.path (ph√≤ng khi t√°ch code)\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"‚úÖ PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"‚úÖ Copy PDF v√†o:\", DATA_DIR)\n",
    "print(\"‚úÖ (Optional) Copy PDF kh√°c v√†o:\", CUSTOM_DIR)\n",
    "print(\"‚úÖ Chroma DB l∆∞u ·ªü:\", CHROMA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496a4f6",
   "metadata": {},
   "source": [
    "<h3>Check d·ªØ li·ªáu PDF ƒë√£ c√≥ ch∆∞a</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cacaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "pdf_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n",
    "print(\"üìÑ S·ªë PDF trong generative_ai:\", len(pdf_files))\n",
    "for f in pdf_files[:20]:\n",
    "    print(\" -\", os.path.basename(f))\n",
    "\n",
    "if len(pdf_files) == 0:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Ch∆∞a c√≥ PDF!\\n\"\n",
    "        f\"H√£y copy v√†i file .pdf v√†o folder:\\n{DATA_DIR}\\n\"\n",
    "        \"R·ªìi ch·∫°y l·∫°i cell n√†y.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917e761",
   "metadata": {},
   "source": [
    "<H3>Clean text + Loader + Chunking</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d456075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def clean_vietnamese_text(text: str) -> str:\n",
    "    # Chu·∫©n h√≥a Unicode ti·∫øng Vi·ªát\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    # Lo·∫°i b·ªè k√Ω t·ª± control (gi·ªØ \\n \\t)\n",
    "    text = \"\".join(\n",
    "        ch for ch in text\n",
    "        if (not unicodedata.category(ch).startswith(\"C\")) or ch in \"\\n\\t\"\n",
    "    )\n",
    "\n",
    "    # G·ªôp kho·∫£ng tr·∫Øng th·ª´a\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "class SimpleLoader:\n",
    "    def load_pdf(self, pdf_file: str):\n",
    "        docs = PyPDFLoader(pdf_file, extract_images=True).load()\n",
    "        for doc in docs:\n",
    "            doc.page_content = clean_vietnamese_text(doc.page_content)\n",
    "            # th√™m metadata ƒë·ªÉ debug (file name + page)\n",
    "            doc.metadata[\"source_file\"] = os.path.basename(pdf_file)\n",
    "        return docs\n",
    "\n",
    "    def load_dir(self, dir_path: str) -> List:\n",
    "        pdfs = sorted(glob.glob(os.path.join(dir_path, \"*.pdf\")))\n",
    "        if not pdfs:\n",
    "            raise ValueError(f\"No PDF files found in: {dir_path}\")\n",
    "\n",
    "        all_docs = []\n",
    "        for pdf in tqdm(pdfs, desc=\"Loading PDFs\"):\n",
    "            try:\n",
    "                all_docs.extend(self.load_pdf(pdf))\n",
    "            except Exception as e:\n",
    "                print(\"Skip:\", pdf, \"|\", e)\n",
    "        return all_docs\n",
    "\n",
    "class TextSplitter:\n",
    "    def __init__(self, chunk_size: int = 400, chunk_overlap: int = 120):\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "\n",
    "    def split(self, documents):\n",
    "        return self.splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adcf0c",
   "metadata": {},
   "source": [
    "<H3>Vector DB (Chroma + Embeddings)</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(\n",
    "        self,\n",
    "        documents=None,\n",
    "        embedding_model: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        collection_name: str = \"vietnamese_docs\",\n",
    "        persist_dir: str = None,\n",
    "    ):\n",
    "        self.persist_dir = persist_dir or CHROMA_DIR\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        self.embedding = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "        self.db = self._build_db(documents)\n",
    "\n",
    "    def _build_db(self, documents):\n",
    "        if documents is None or len(documents) == 0:\n",
    "            # load existing\n",
    "            return Chroma(\n",
    "                collection_name=self.collection_name,\n",
    "                embedding_function=self.embedding,\n",
    "                persist_directory=self.persist_dir,\n",
    "            )\n",
    "        else:\n",
    "            # build new\n",
    "            return Chroma.from_documents(\n",
    "                documents=documents,\n",
    "                embedding=self.embedding,\n",
    "                collection_name=self.collection_name,\n",
    "                persist_directory=self.persist_dir,\n",
    "            )\n",
    "\n",
    "    def get_retriever(self, k: int = 4):\n",
    "        return self.db.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": k},\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee973c9",
   "metadata": {},
   "source": [
    "<H3>LLM (Qwen) + fallback model nh·ªè cho m√°y y·∫øu</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93746b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "def get_hf_llm(\n",
    "    model_name: str = \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    temperature: float = 0.2,\n",
    "    max_new_tokens: int = 450,\n",
    "):\n",
    "    # N·∫øu m√°y y·∫øu / kh√¥ng GPU -> d√πng model nh·ªè cho ch·∫Øc\n",
    "    if not torch.cuda.is_available() and model_name == \"Qwen/Qwen2.5-3B-Instruct\":\n",
    "        model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng th·∫•y GPU -> auto d√πng model nh·ªè:\", model_name)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    gen_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_p=0.75,\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=gen_pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdac6e7",
   "metadata": {},
   "source": [
    "<H3>Prompt + Parser + RAG chain (k√®m hi·ªÉn th·ªã context)</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de1bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class FocusedAnswerParser(StrOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        text = (text or \"\").strip()\n",
    "\n",
    "        if \"[TR·∫¢ L·ªúI]:\" in text:\n",
    "            answer = text.split(\"[TR·∫¢ L·ªúI]:\")[-1].strip()\n",
    "        else:\n",
    "            answer = text\n",
    "\n",
    "        answer = re.sub(r\"\\n+\", \" \", answer).strip()\n",
    "\n",
    "        # gi·ªõi h·∫°n 3-5 c√¢u\n",
    "        parts = [p.strip() for p in re.split(r\"(?<=[\\.\\!\\?])\\s+\", answer) if p.strip()]\n",
    "        if len(parts) > 5:\n",
    "            answer = \" \".join(parts[:5]) + \" ...\"\n",
    "        return answer\n",
    "\n",
    "class OfflineRAG:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.prompt = PromptTemplate.from_template(\"\"\"\n",
    "B·∫°n l√† tr·ª£ l√Ω AI ph√¢n t√≠ch t√†i li·ªáu ti·∫øng Vi·ªát.\n",
    "\n",
    "[T√ÄI LI·ªÜU]:\n",
    "{context}\n",
    "\n",
    "[C√ÇU H·ªéI]:\n",
    "{question}\n",
    "\n",
    "H√£y tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu. N·∫øu t√†i li·ªáu kh√¥ng c√≥ th√¥ng tin, n√≥i r√µ \"Kh√¥ng c√≥ th√¥ng tin\".\n",
    "Tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß th√¥ng tin (3-5 c√¢u chi ti·∫øt), kh√¥ng th√™m b·∫•t k·ª≥ chi ti·∫øt n√†o ngo√†i t√†i li·ªáu.\n",
    "\n",
    "[TR·∫¢ L·ªúI]:\n",
    "\"\"\".strip())\n",
    "        self.answer_parser = FocusedAnswerParser()\n",
    "\n",
    "    def get_chain(self, retriever):\n",
    "        def format_docs(docs):\n",
    "            # tr·∫£ v·ªÅ context + metadata cho demo\n",
    "            blocks = []\n",
    "            seen = set()\n",
    "            for d in docs:\n",
    "                content = (d.page_content or \"\").strip()\n",
    "                if not content or len(content) < 40:\n",
    "                    continue\n",
    "                key = content[:200]\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                seen.add(key)\n",
    "                src = d.metadata.get(\"source_file\", \"unknown\")\n",
    "                page = d.metadata.get(\"page\", \"?\")\n",
    "                blocks.append(f\"[{src} | page {page}]\\n{content}\")\n",
    "            return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "        chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | self.answer_parser\n",
    "        )\n",
    "        return chain\n",
    "\n",
    "    def get_context_only(self, retriever):\n",
    "        # ti·ªán ƒë·ªÉ show context ri√™ng tr√™n UI\n",
    "        def format_docs(docs):\n",
    "            blocks = []\n",
    "            for d in docs:\n",
    "                src = d.metadata.get(\"source_file\", \"unknown\")\n",
    "                page = d.metadata.get(\"page\", \"?\")\n",
    "                blocks.append(f\"[{src} | page {page}]\\n{(d.page_content or '').strip()}\")\n",
    "            return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "        return retriever | format_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f5aa1",
   "metadata": {},
   "source": [
    "<H3>Build pipeline: load ‚Üí chunk ‚Üí vector ‚Üí chain</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918fee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load + Chunk\n",
    "loader = SimpleLoader()\n",
    "splitter = TextSplitter(chunk_size=400, chunk_overlap=120)\n",
    "\n",
    "raw_docs = loader.load_dir(DATA_DIR)\n",
    "split_docs = splitter.split(raw_docs)\n",
    "\n",
    "print(\"‚úÖ Raw docs:\", len(raw_docs))\n",
    "print(\"‚úÖ Chunks:\", len(split_docs))\n",
    "\n",
    "# 2) Vector DB + retriever\n",
    "vdb = VectorDB(documents=split_docs)\n",
    "retriever = vdb.get_retriever(k=4)\n",
    "\n",
    "# 3) LLM + RAG\n",
    "llm = get_hf_llm()\n",
    "rag = OfflineRAG(llm)\n",
    "rag_chain = rag.get_chain(retriever)\n",
    "ctx_chain = rag.get_context_only(retriever)\n",
    "\n",
    "def answer_question(question: str) -> str:\n",
    "    try:\n",
    "        return rag_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_context(question: str) -> str:\n",
    "    try:\n",
    "        return ctx_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test nhanh\n",
    "print(answer_question(\"T√†i li·ªáu c√≥ n√≥i v√™ÃÄ lu√¢Ã£t giao th√¥ng kh√¥ng?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ffce31",
   "metadata": {},
   "source": [
    "<H3>Gradio UI</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebcccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks(title=\"RAG: H√äÃ£ TH√îÃÅNG HOÃâI ƒêAÃÅP LU√ÇÃ£T GIAO TH√îNG\") as demo:\n",
    "    gr.Markdown(\"# üìå RAG ‚Äì H√äÃ£ TH√îÃÅNG HOÃâI ƒêAÃÅP LU√ÇÃ£T GIAO TH√îNG\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            question = gr.Textbox(\n",
    "                label=\"C√¢u h·ªèi\",\n",
    "                placeholder=\"Nh·∫≠p c√¢u h·ªèi v·ªÅ n·ªôi dung trong PDF...\",\n",
    "                lines=3\n",
    "            )\n",
    "            btn = gr.Button(\"G·ª≠i\", variant=\"primary\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            answer = gr.Textbox(label=\"C√¢u tr·∫£ l·ªùi\", lines=6, interactive=False)\n",
    "\n",
    "    gr.Markdown(\"## üîé Context (Top-k chunks h·ªá th·ªëng l·∫•y ra)\")\n",
    "    context = gr.Textbox(label=\"Top-k Context\", lines=10, interactive=False)\n",
    "\n",
    "    def qa_with_ctx(q):\n",
    "        return answer_question(q), get_context(q)\n",
    "\n",
    "    btn.click(fn=qa_with_ctx, inputs=question, outputs=[answer, context])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457224a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a771cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a105dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
