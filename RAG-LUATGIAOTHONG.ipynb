{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa50bd8",
   "metadata": {},
   "source": [
    "<H3>Install libs<H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4233ff37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mtl/DATA 6TB5/PROJECT AI/RAG-NLP/.venv/bin/pip: 2: exec: /media/mtl/DATA 6TB/PROJECT AI/RAG-NLP/.venv/bin/python: not found\n"
     ]
    }
   ],
   "source": [
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán ch√≠nh cho m√¥ h√¨nh ng√¥n ng·ªØ, embedding, RAG v√† giao di·ªán web\n",
    "!pip install -q \\\n",
    "  \"torch>=2.0.0\" \\\n",
    "  \"transformers>=4.40.0\" \\\n",
    "  \"accelerate>=0.30.0\" \\\n",
    "  \"huggingface-hub>=0.23.0\" \\\n",
    "  \"sentence-transformers>=2.7.0\" \\\n",
    "  \"langchain>=0.2.0\" \\\n",
    "  \"langchain-core>=0.2.0\" \\\n",
    "  \"langchain-community>=0.1.0\" \\\n",
    "  \"langchain-text-splitters>=0.2.0\" \\\n",
    "  \"chromadb>=0.5.0\" \\\n",
    "  \"langchain-chroma>=0.2.0\" \\\n",
    "  \"pypdf>=4.2.0\" \\\n",
    "  \"gradio>=5.0.0\" \\\n",
    "  \"langchain-huggingface\" \\\n",
    "  \"wget\" \\\n",
    "  \"tqdm\" \\\n",
    "  \"ipywidgets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "153d55dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bbe7be",
   "metadata": {},
   "source": [
    "<H3>Setup project + t·∫°o c·∫•u tr√∫c th∆∞ m·ª•c</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef1e955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PROJECT_ROOT: /media/mtl/DATA 6TB5/PROJECT AI/RAG-NLP/rag_langchain\n",
      "‚úÖ Copy PDF v√†o: /media/mtl/DATA 6TB5/PROJECT AI/RAG-NLP/rag_langchain/data_source/generative_ai\n",
      "‚úÖ (Optional) Copy PDF kh√°c v√†o: /media/mtl/DATA 6TB5/PROJECT AI/RAG-NLP/rag_langchain/data_source/custom\n",
      "‚úÖ Chroma DB l∆∞u ·ªü: /media/mtl/DATA 6TB5/PROJECT AI/RAG-NLP/rag_langchain/chroma_data\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Root d·ª± √°n: d√πng folder \"rag_langchain\" n·∫±m c√πng c·∫•p notebook (nh∆∞ ·∫£nh b·∫°n)\n",
    "PROJECT_ROOT = os.path.abspath(\"rag_langchain\")\n",
    "\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data_source\", \"generative_ai\")  # b·∫°n copy PDF v√†o ƒë√¢y\n",
    "CUSTOM_DIR = os.path.join(PROJECT_ROOT, \"data_source\", \"custom\")       # tu·ª≥ ch·ªçn\n",
    "CHROMA_DIR = os.path.join(PROJECT_ROOT, \"chroma_data\")                 # l∆∞u vector DB\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CUSTOM_DIR, exist_ok=True)\n",
    "os.makedirs(CHROMA_DIR, exist_ok=True)\n",
    "\n",
    "# src (tu·ª≥ ch·ªçn, cho ƒë√∫ng c·∫•u tr√∫c t√†i li·ªáu)\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, \"src\", \"base\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, \"src\", \"rag\"), exist_ok=True)\n",
    "\n",
    "# t·∫°o __init__.py\n",
    "for p in [\n",
    "    os.path.join(PROJECT_ROOT, \"src\", \"__init__.py\"),\n",
    "    os.path.join(PROJECT_ROOT, \"src\", \"base\", \"__init__.py\"),\n",
    "    os.path.join(PROJECT_ROOT, \"src\", \"rag\", \"__init__.py\"),\n",
    "]:\n",
    "    if not os.path.exists(p):\n",
    "        open(p, \"w\", encoding=\"utf-8\").close()\n",
    "\n",
    "# th√™m PROJECT_ROOT v√†o sys.path (ph√≤ng khi t√°ch code)\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"‚úÖ PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"‚úÖ Copy PDF v√†o:\", DATA_DIR)\n",
    "print(\"‚úÖ (Optional) Copy PDF kh√°c v√†o:\", CUSTOM_DIR)\n",
    "print(\"‚úÖ Chroma DB l∆∞u ·ªü:\", CHROMA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496a4f6",
   "metadata": {},
   "source": [
    "<h3>Check d·ªØ li·ªáu PDF ƒë√£ c√≥ ch∆∞a</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30cacaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ S·ªë PDF trong generative_ai: 8\n",
      " - 05-vbhn-bgtvt.pdf\n",
      " - 168-nd-cp.signed.pdf\n",
      " - 2023_1335 + 1336_12-VBHN-BGTVT.pdf\n",
      " - 35-2024-qh15.pdf\n",
      " - 36-2024-qh15.pdf\n",
      " - 36-2024-qh15_tiep.pdf\n",
      " - 51-bgtvt-kem.pdf\n",
      " - 73-bca.pdf\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "pdf_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n",
    "print(\"üìÑ S·ªë PDF trong generative_ai:\", len(pdf_files))\n",
    "for f in pdf_files[:20]:\n",
    "    print(\" -\", os.path.basename(f))\n",
    "\n",
    "if len(pdf_files) == 0:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Ch∆∞a c√≥ PDF!\\n\"\n",
    "        f\"H√£y copy v√†i file .pdf v√†o folder:\\n{DATA_DIR}\\n\"\n",
    "        \"R·ªìi ch·∫°y l·∫°i cell n√†y.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917e761",
   "metadata": {},
   "source": [
    "<H3>Clean text + Loader + Chunking</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d456075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def clean_vietnamese_text(text: str) -> str:\n",
    "    # Chu·∫©n h√≥a Unicode ti·∫øng Vi·ªát\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    # Lo·∫°i b·ªè k√Ω t·ª± control (gi·ªØ \\n \\t)\n",
    "    text = \"\".join(\n",
    "        ch for ch in text\n",
    "        if (not unicodedata.category(ch).startswith(\"C\")) or ch in \"\\n\\t\"\n",
    "    )\n",
    "\n",
    "    # G·ªôp kho·∫£ng tr·∫Øng th·ª´a\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "class SimpleLoader:\n",
    "    def load_pdf(self, pdf_file: str):\n",
    "        docs = PyPDFLoader(pdf_file, extract_images=True).load()\n",
    "        for doc in docs:\n",
    "            doc.page_content = clean_vietnamese_text(doc.page_content)\n",
    "            # th√™m metadata ƒë·ªÉ debug (file name + page)\n",
    "            doc.metadata[\"source_file\"] = os.path.basename(pdf_file)\n",
    "        return docs\n",
    "\n",
    "    def load_dir(self, dir_path: str) -> List:\n",
    "        pdfs = sorted(glob.glob(os.path.join(dir_path, \"*.pdf\")))\n",
    "        if not pdfs:\n",
    "            raise ValueError(f\"No PDF files found in: {dir_path}\")\n",
    "\n",
    "        all_docs = []\n",
    "        for pdf in tqdm(pdfs, desc=\"Loading PDFs\"):\n",
    "            try:\n",
    "                all_docs.extend(self.load_pdf(pdf))\n",
    "            except Exception as e:\n",
    "                print(\"Skip:\", pdf, \"|\", e)\n",
    "        return all_docs\n",
    "\n",
    "class TextSplitter:\n",
    "    def __init__(self, chunk_size: int = 400, chunk_overlap: int = 120):\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "\n",
    "    def split(self, documents):\n",
    "        return self.splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adcf0c",
   "metadata": {},
   "source": [
    "<H3>Vector DB (Chroma + Embeddings) - H·ªó tr·ª£ ‚Äúth√™m t√†i li·ªáu‚Äù (incremental)</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8474f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(\n",
    "        self,\n",
    "        documents=None,\n",
    "        #embedding_model: str = \"dangvantuan/vietnamese-document-embedding\",\n",
    "        embedding_model: str = \"keepitreal/vietnamese-sbert\",\n",
    "        collection_name: str = \"vietnamese_docs\",\n",
    "        persist_dir: str = None,\n",
    "    ):\n",
    "        self.persist_dir = persist_dir or CHROMA_DIR\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        # ‚úÖ B·∫≠t trust_remote_code cho model c√≥ custom code\n",
    "        self.embedding = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model,\n",
    "            model_kwargs={\"trust_remote_code\": True},\n",
    "        )\n",
    "\n",
    "        self.db = Chroma(\n",
    "            collection_name=self.collection_name,\n",
    "            embedding_function=self.embedding,\n",
    "            persist_directory=self.persist_dir,\n",
    "        )\n",
    "\n",
    "        if documents and len(documents) > 0:\n",
    "            self.add_documents(documents)\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        self.db.add_documents(documents)\n",
    "        if hasattr(self.db, \"persist\"):\n",
    "            try:\n",
    "                self.db.persist()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    '''\n",
    "    #similarity th∆∞·ªùng\n",
    "    def get_retriever(self, k: int = 4):\n",
    "        return self.db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    '''\n",
    "\n",
    "    #MMR ƒë·ªÉ b·ªõt tr√πng ƒëo·∫°n + ph·ªß √Ω r·ªông h∆°n:\n",
    "    def get_retriever(self, k: int = 8):\n",
    "        return self.db.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": k, \"fetch_k\": max(20, k*4), \"lambda_mult\": 0.2}\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee973c9",
   "metadata": {},
   "source": [
    "<H3>LLM (Qwen) + fallback model nh·ªè cho m√°y y·∫øu</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93746b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "def get_hf_llm(\n",
    "    model_name: str = \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    temperature: float = 0.2, # th√¢ÃÅp thiÃÄ ƒë√¥Ãâ lan man = chiÃÅnh xaÃÅc h∆°n\n",
    "    max_new_tokens: int = 900,\n",
    "):\n",
    "    # N·∫øu m√°y y·∫øu / kh√¥ng GPU -> d√πng model nh·ªè cho ch·∫Øc\n",
    "    if not torch.cuda.is_available() and model_name == \"Qwen/Qwen2.5-3B-Instruct\":\n",
    "        model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng th·∫•y GPU -> auto d√πng model nh·ªè:\", model_name)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    gen_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_p=0.75,\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=gen_pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdac6e7",
   "metadata": {},
   "source": [
    "<H3>Prompt + Parser + RAG chain (k√®m hi·ªÉn th·ªã context)</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32de1bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class FocusedAnswerParser(StrOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        text = (text or \"\").strip()\n",
    "\n",
    "        m = re.search(r\"\\[?\\s*TR·∫¢\\s*L·ªúI\\s*\\]?\\s*[:Ôºö]\\s*\", text, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            text = text[m.end():].strip()\n",
    "\n",
    "        text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)\n",
    "        text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
    "        return text\n",
    "\n",
    "class OfflineRAG:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.prompt = PromptTemplate.from_template(\"\"\"\n",
    "B·∫°n l√† tr·ª£ l√Ω AI tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu ph√°p lu·∫≠t ti·∫øng Vi·ªát.\n",
    "\n",
    "[T√ÄI LI·ªÜU]:\n",
    "{context}\n",
    "\n",
    "[C√ÇU H·ªéI]:\n",
    "{question}\n",
    "\n",
    "QUY T·∫ÆC:\n",
    "1) CH·ªà d√πng th√¥ng tin c√≥ trong [T√ÄI LI·ªÜU]. Kh√¥ng suy di·ªÖn, kh√¥ng th√™m ki·∫øn th·ª©c ngo√†i.\n",
    "2) N·∫øu t√†i li·ªáu KH√îNG c√≥ n·ªôi dung h·ªèi t·ªõi: n√≥i r√µ \"Kh√¥ng t√¨m th·∫•y trong c√°c ƒëo·∫°n tr√≠ch hi·ªán c√≥\" v√† g·ª£i √Ω ng∆∞·ªùi d√πng n·∫°p ƒë√∫ng vƒÉn b·∫£n.\n",
    "3) Tr·∫£ l·ªùi theo ƒë√∫ng m·∫´u d∆∞·ªõi ƒë√¢y. M·ªói √Ω ph·∫£i k√®m ngu·ªìn (file | page).\n",
    "\n",
    "M·∫™U TR·∫¢ L·ªúI (b·∫Øt bu·ªôc):\n",
    "- T√≥m t·∫Øt nhanh (2‚Äì4 g·∫°ch ƒë·∫ßu d√≤ng).\n",
    "- Tr√≠ch √Ω theo ƒêi·ªÅu/Kho·∫£n (n·∫øu t√†i li·ªáu c√≥):\n",
    "  * ƒêi·ªÅu X (n·∫øu c√≥) ‚Äî √ù ch√≠nh: ...\n",
    "    - Kho·∫£n ...: ...\n",
    "    - (Ngu·ªìn: file | page)\n",
    "- K·∫øt lu·∫≠n: 1‚Äì2 c√¢u.\n",
    "\n",
    "[TR·∫¢ L·ªúI]:\n",
    "\"\"\".strip())\n",
    "        self.answer_parser = FocusedAnswerParser()\n",
    "\n",
    "    def get_chain(self, retriever):\n",
    "        def format_docs(docs):\n",
    "            blocks = []\n",
    "            seen = set()\n",
    "            for d in docs:\n",
    "                content = (d.page_content or \"\").strip()\n",
    "                if len(content) < 40:\n",
    "                    continue\n",
    "                src = d.metadata.get(\"source_file\", \"unknown\")\n",
    "                page = d.metadata.get(\"page\", \"?\")\n",
    "\n",
    "                key = (src, page, hash(content))\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                seen.add(key)\n",
    "\n",
    "                blocks.append(f\"[{src} | page {page}]\\n{content}\")\n",
    "            return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "        return (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | self.answer_parser\n",
    "        )\n",
    "\n",
    "    def get_context_only(self, retriever):\n",
    "        def format_docs(docs):\n",
    "            blocks = []\n",
    "            for d in docs:\n",
    "                src = d.metadata.get(\"source_file\", \"unknown\")\n",
    "                page = d.metadata.get(\"page\", \"?\")\n",
    "                blocks.append(f\"[{src} | page {page}]\\n{(d.page_content or '').strip()}\")\n",
    "            return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "        return retriever | format_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f5aa1",
   "metadata": {},
   "source": [
    "<H3>Build pipeline: load ‚Üí chunk ‚Üí vector ‚Üí chain</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf1d89c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5710e1ca3d7e4f79a035486fe092fc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d628ba456c4c8092a39964d16d2e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c966ec7d98c487f8f4a8a679345d896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9081fd98c1f64a7ba2de1ddc93bd90f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c684fc657f348b7a09a1f3b40500c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/752 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4f4ea969d94fc2945cb179b4e4ae80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404e145be0e84c3c9e68bf7a5f507927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1acb3269d084ac9bc22aeb7f0ed3ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ae75c63e1f48b489bcccbb36152fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc08986fba94923b32e1ba898e8e1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6a81420a3f42a092c54d64526ad8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/17.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f1fa8f9c13491ab44308c51d2f7401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ffcb093e764562aa47791af48913c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca395477bc64c34affe3b93ee47431a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dc12ff7f7242b0b0de00d9dafbc822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a503761992f43cdad1dcfa36263244b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Loading PDFs:  12%|‚ñà‚ñé        | 1/8 [00:01<00:10,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip: /media/mtl/DATA 6TB5/PROJECT AI/RAG-NLP/rag_langchain/data_source/generative_ai/168-nd-cp.signed.pdf | cannot reshape array of size 41654 into shape (2357,1670,newaxis)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDFs:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:05<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip: /media/mtl/DATA 6TB5/PROJECT AI/RAG-NLP/rag_langchain/data_source/generative_ai/51-bgtvt-kem.pdf | cannot reshape array of size 2819 into shape (93,97,newaxis)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:08<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ready. PDFs in kho: 8\n"
     ]
    }
   ],
   "source": [
    "# 1) Loader + Splitter\n",
    "loader = SimpleLoader()\n",
    "splitter = TextSplitter(chunk_size=400, chunk_overlap=120)\n",
    "\n",
    "# 2) VectorDB (persist)\n",
    "vdb = VectorDB(documents=None)  # m·ªü DB t·ª´ persist_dir\n",
    "\n",
    "# N·∫øu mu·ªën rebuild s·∫°ch DB m·ªói l·∫ßn ch·∫°y notebook, uncomment block d∆∞·ªõi:\n",
    "# import shutil\n",
    "# if os.path.exists(CHROMA_DIR):\n",
    "#     shutil.rmtree(CHROMA_DIR, ignore_errors=True)\n",
    "# vdb = VectorDB(documents=None)\n",
    "\n",
    "# 3) LLM + RAG\n",
    "llm = get_hf_llm()\n",
    "rag = OfflineRAG(llm)\n",
    "\n",
    "# 4) N·∫øu trong folder ƒë√£ c√≥ PDF th√¨ ingest 1 l·∫ßn (ƒë·ªÉ c√≥ d·ªØ li·ªáu)\n",
    "pdfs = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n",
    "if len(pdfs) > 0:\n",
    "    raw_docs = loader.load_dir(DATA_DIR)\n",
    "    split_docs = splitter.split(raw_docs)\n",
    "    vdb.add_documents(split_docs)\n",
    "\n",
    "retriever = vdb.get_retriever(k=10)\n",
    "rag_chain = rag.get_chain(retriever)\n",
    "ctx_chain = rag.get_context_only(retriever)\n",
    "\n",
    "def answer_question(question: str) -> str:\n",
    "    try:\n",
    "        return rag_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_context(question: str) -> str:\n",
    "    try:\n",
    "        return ctx_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Ready. PDFs in kho:\", len(glob.glob(os.path.join(DATA_DIR, '*.pdf'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3097de",
   "metadata": {},
   "source": [
    "<h3>‚Äúingest pipeline‚Äù cho file upload (copy ‚Üí load ‚Üí chunk ‚Üí add ‚Üí refresh chain)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b3b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def list_pdfs_md(data_dir: str) -> str:\n",
    "    pdfs = sorted(glob.glob(os.path.join(data_dir, \"*.pdf\")))\n",
    "    if not pdfs:\n",
    "        return \"*(Ch∆∞a c√≥ file PDF n√†o trong kho)*\"\n",
    "    lines = [f\"- {os.path.basename(p)}\" for p in pdfs]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def ingest_uploaded_pdfs(uploaded_files, loader, splitter, vdb, data_dir: str):\n",
    "    \"\"\"\n",
    "    uploaded_files: list[gradio UploadedFile] ho·∫∑c list[path]\n",
    "    - copy v√†o data_dir\n",
    "    - load -> chunk\n",
    "    - add v√†o Chroma\n",
    "    \"\"\"\n",
    "    if not uploaded_files:\n",
    "        return \"‚ùå B·∫°n ch∆∞a ch·ªçn file PDF n√†o.\"\n",
    "\n",
    "    saved = []\n",
    "    for f in uploaded_files:\n",
    "        # gr.File c√≥ th·ªÉ tr·∫£ object c√≥ thu·ªôc t√≠nh .name ho·∫∑c l√† string path\n",
    "        src_path = getattr(f, \"name\", None) or str(f)\n",
    "        base = os.path.basename(src_path)\n",
    "        dst_path = os.path.join(data_dir, base)\n",
    "\n",
    "        # copy v√†o kho\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        saved.append(dst_path)\n",
    "\n",
    "    # load + chunk ch·ªâ nh·ªØng file m·ªõi\n",
    "    new_docs = []\n",
    "    for p in saved:\n",
    "        try:\n",
    "            new_docs.extend(loader.load_pdf(p))\n",
    "        except Exception as e:\n",
    "            print(\"Skip load:\", p, e)\n",
    "\n",
    "    if not new_docs:\n",
    "        return \"‚ö†Ô∏è Copy xong nh∆∞ng kh√¥ng ƒë·ªçc ƒë∆∞·ª£c PDF (c√≥ th·ªÉ PDF scan ·∫£nh / l·ªói ƒë·ªãnh d·∫°ng).\"\n",
    "\n",
    "    new_chunks = splitter.split(new_docs)\n",
    "    if not new_chunks:\n",
    "        return \"‚ö†Ô∏è ƒê·ªçc ƒë∆∞·ª£c nh∆∞ng chunk r·ªóng (PDF c√≥ th·ªÉ to√†n ·∫£nh).\"\n",
    "\n",
    "    # add v√†o Vector DB\n",
    "    vdb.add_documents(new_chunks)\n",
    "\n",
    "    return f\"‚úÖ ƒê√£ n·∫°p {len(saved)} file | pages loaded: {len(new_docs)} | chunks added: {len(new_chunks)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ffce31",
   "metadata": {},
   "source": [
    "<H3>Gradio UI</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a356620f",
   "metadata": {},
   "source": [
    "<H4>Ch·ªëng tr√πng ingest b·∫±ng HASH + ch·ªâ ingest file m·ªõi<H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e692680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, hashlib, shutil, glob\n",
    "from datetime import datetime\n",
    "\n",
    "HASH_DB_PATH = os.path.join(PROJECT_ROOT, \"ingested_hashes.json\")\n",
    "\n",
    "def load_hash_db():\n",
    "    if os.path.exists(HASH_DB_PATH):\n",
    "        try:\n",
    "            with open(HASH_DB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_hash_db(db):\n",
    "    with open(HASH_DB_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(db, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def sha256_file(path, chunk_size=1024*1024):\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "HASH_DB = load_hash_db()\n",
    "\n",
    "def ingest_uploaded_pdfs_no_dup(uploaded_files, loader, splitter, vdb, data_dir: str):\n",
    "    \"\"\"\n",
    "    - copy file v√†o kho\n",
    "    - t√≠nh hash\n",
    "    - n·∫øu hash ƒë√£ c√≥ => skip\n",
    "    - n·∫øu m·ªõi => load->chunk->add_documents\n",
    "    \"\"\"\n",
    "    if not uploaded_files:\n",
    "        return \"‚ùå B·∫°n ch∆∞a ch·ªçn file PDF n√†o.\"\n",
    "\n",
    "    added_files, skipped_files = [], []\n",
    "    new_docs_total, new_chunks_total = 0, 0\n",
    "\n",
    "    for f in uploaded_files:\n",
    "        src_path = getattr(f, \"name\", None) or str(f)\n",
    "        base = os.path.basename(src_path)\n",
    "        dst_path = os.path.join(data_dir, base)\n",
    "\n",
    "        # copy v√†o kho (n·∫øu tr√πng t√™n th√¨ th√™m timestamp)\n",
    "        if os.path.exists(dst_path):\n",
    "            name, ext = os.path.splitext(base)\n",
    "            dst_path = os.path.join(data_dir, f\"{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}{ext}\")\n",
    "\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "        file_hash = sha256_file(dst_path)\n",
    "\n",
    "        if file_hash in HASH_DB:\n",
    "            skipped_files.append(os.path.basename(dst_path))\n",
    "            # file tr√πng hash th√¨ x√≥a b·∫£n copy v·ª´a t·∫°o ƒë·ªÉ kho s·∫°ch\n",
    "            try:\n",
    "                os.remove(dst_path)\n",
    "            except:\n",
    "                pass\n",
    "            continue\n",
    "\n",
    "        # file m·ªõi\n",
    "        try:\n",
    "            docs = loader.load_pdf(dst_path)\n",
    "        except Exception as e:\n",
    "            # n·∫øu kh√¥ng ƒë·ªçc ƒë∆∞·ª£c th√¨ b·ªè v√† x√≥a\n",
    "            try: os.remove(dst_path)\n",
    "            except: pass\n",
    "            continue\n",
    "\n",
    "        chunks = splitter.split(docs)\n",
    "        if chunks:\n",
    "            vdb.add_documents(chunks)\n",
    "            HASH_DB[file_hash] = {\n",
    "                \"file\": os.path.basename(dst_path),\n",
    "                \"added_at\": datetime.now().isoformat()\n",
    "            }\n",
    "            added_files.append(os.path.basename(dst_path))\n",
    "            new_docs_total += len(docs)\n",
    "            new_chunks_total += len(chunks)\n",
    "        else:\n",
    "            # chunk r·ªóng th√¨ b·ªè\n",
    "            try: os.remove(dst_path)\n",
    "            except: pass\n",
    "\n",
    "    save_hash_db(HASH_DB)\n",
    "\n",
    "    msg = f\"‚úÖ Added: {len(added_files)} file(s), pages: {new_docs_total}, chunks: {new_chunks_total}\\n\"\n",
    "    if added_files:\n",
    "        msg += \"  + \" + \", \".join(added_files) + \"\\n\"\n",
    "    if skipped_files:\n",
    "        msg += f\"‚ö†Ô∏è Skipped (duplicate by hash): {len(skipped_files)}\\n\"\n",
    "        msg += \"  - \" + \", \".join(skipped_files)\n",
    "\n",
    "    return msg.strip()\n",
    "\n",
    "def list_pdf_names(data_dir):\n",
    "    return [os.path.basename(p) for p in sorted(glob.glob(os.path.join(data_dir, \"*.pdf\")))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302ed06",
   "metadata": {},
   "source": [
    "<H4>Top-k k√®m similarity score (Chroma)</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf00782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_scores(query: str, k: int = 4):\n",
    "    \"\"\"\n",
    "    Tr·∫£ v·ªÅ (doc, score). V·ªõi Chroma: score th∆∞·ªùng l√† distance (th·∫•p h∆°n = gi·ªëng h∆°n)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pairs = vdb.db.similarity_search_with_score(query, k=k)\n",
    "        return pairs\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def format_scored_context(pairs):\n",
    "    if not pairs:\n",
    "        return \"Kh√¥ng l·∫•y ƒë∆∞·ª£c context/score.\"\n",
    "    blocks = []\n",
    "    for doc, score in pairs:\n",
    "        src = doc.metadata.get(\"source_file\", \"unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        text = (doc.page_content or \"\").strip()\n",
    "        text = text[:1200]  # c·∫Øt b·ªõt cho UI ƒë·ª° d√†i\n",
    "        blocks.append(f\"[{src} | page {page} | score={score:.4f}]\\n{text}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "def answer_with_scored_ctx(q: str):\n",
    "    ans = answer_question(q)\n",
    "    pairs = retrieve_with_scores(q, k=6)\n",
    "    ctx = format_scored_context(pairs)\n",
    "    return ans, ctx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422330d0",
   "metadata": {},
   "source": [
    "<H4>UI: click PDF ƒë·ªÉ ‚Äúm·ªü/t·∫£i‚Äù + upload ingest ch·ªëng tr√πng + context c√≥ score</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc729a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://cfa5373e1f0f39cba4.gradio.live\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cfa5373e1f0f39cba4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def ui_refresh_choices():\n",
    "    names = list_pdf_names(DATA_DIR)\n",
    "    return gr.update(choices=names, value=names[0] if names else None)\n",
    "\n",
    "def ui_open_file(selected_name):\n",
    "    if not selected_name:\n",
    "        return None\n",
    "    path = os.path.join(DATA_DIR, selected_name)\n",
    "    return path if os.path.exists(path) else None\n",
    "\n",
    "def ui_upload_and_ingest(files):\n",
    "    msg = ingest_uploaded_pdfs_no_dup(files, loader, splitter, vdb, DATA_DIR)\n",
    "\n",
    "    # refresh chain (ƒë·ªÉ ch·∫Øc k√®o)\n",
    "    global retriever, rag_chain, ctx_chain\n",
    "    retriever = vdb.get_retriever(k=4)\n",
    "    rag_chain = rag.get_chain(retriever)\n",
    "    ctx_chain = rag.get_context_only(retriever)\n",
    "\n",
    "    # refresh list & open first file\n",
    "    names = list_pdf_names(DATA_DIR)\n",
    "    first = names[0] if names else None\n",
    "    return msg, ui_refresh_choices(), ui_open_file(first)\n",
    "\n",
    "METHOD_MD = \"\"\"\n",
    "## üß† Ph∆∞∆°ng ph√°p & c√°ch th·ª©c ho·∫°t ƒë·ªông (Demo RAG)\n",
    "\n",
    "### A. Ingest (N·∫°p PDF)\n",
    "1) Upload PDF ‚Üí copy v√†o kho (`data_source/generative_ai`)\n",
    "2) T√≠nh **SHA256 hash** ‚Üí n·∫øu ƒë√£ c√≥ hash th√¨ **skip** (ch·ªëng tr√πng)\n",
    "3) Tr√≠ch xu·∫•t text t·ª´ PDF (PyPDFLoader)\n",
    "4) L√†m s·∫°ch ti·∫øng Vi·ªát + chunking (chunk_size=400, overlap=120)\n",
    "5) Embedding ‚Üí l∆∞u v√†o ChromaDB (persist)\n",
    "\n",
    "### B. Retrieval + Answer (RAG)\n",
    "1) C√¢u h·ªèi ‚Üí truy v·∫•n Chroma **Top-k similarity**\n",
    "2) L·∫•y c√°c chunk li√™n quan + **score** (ƒë·ªÉ gi·∫£i th√≠ch v√¨ sao n√≥ ch·ªçn ƒëo·∫°n ƒë√≥)\n",
    "3) Nh√©t context v√†o prompt ‚Üí LLM sinh c√¢u tr·∫£ l·ªùi\n",
    "4) UI show context + score ƒë·ªÉ ch·ª©ng minh ‚Äúc√≥ d·∫´n ch·ª©ng‚Äù\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(title=\"RAG - H·ªá th·ªëng h·ªèi ƒë√°p lu·∫≠t giao th√¥ng\") as demo:\n",
    "    gr.Markdown(\"# üìå RAG ‚Äì H·ªÜ TH·ªêNG H·ªéI ƒê√ÅP LU·∫¨T GIAO TH√îNG\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # LEFT\n",
    "        with gr.Column(scale=2):\n",
    "            question = gr.Textbox(label=\"C√¢u h·ªèi\", placeholder=\"Nh·∫≠p c√¢u h·ªèi v·ªÅ n·ªôi dung trong PDF...\", lines=3)\n",
    "            btn = gr.Button(\"G·ª≠i\", variant=\"primary\")\n",
    "            answer = gr.Textbox(label=\"C√¢u tr·∫£ l·ªùi\", lines=14, interactive=False)\n",
    "\n",
    "            gr.Markdown(\"## üîé Context (Top-k chunks + similarity score)\")\n",
    "            context = gr.Textbox(label=\"Top-k Context (score)\", lines=12, interactive=False)\n",
    "\n",
    "            btn.click(fn=answer_with_scored_ctx, inputs=question, outputs=[answer, context])\n",
    "\n",
    "        # RIGHT\n",
    "        with gr.Column(scale=1):\n",
    "            with gr.Tabs():\n",
    "                with gr.Tab(\"üìö T√†i li·ªáu & Upload\"):\n",
    "                    gr.Markdown(\"## üìö Danh s√°ch t√†i li·ªáu ƒë√£ n·∫°p\")\n",
    "                    pdf_picker = gr.Radio(\n",
    "                        choices=list_pdf_names(DATA_DIR),\n",
    "                        label=\"Ch·ªçn t√†i li·ªáu\",\n",
    "                        value=(list_pdf_names(DATA_DIR)[0] if list_pdf_names(DATA_DIR) else None)\n",
    "                    )\n",
    "\n",
    "                    # ‚ÄúM·ªü/t·∫£i‚Äù file\n",
    "                    pdf_file = gr.File(label=\"M·ªü/T·∫£i PDF\", value=ui_open_file(pdf_picker.value))\n",
    "                    pdf_picker.change(fn=ui_open_file, inputs=pdf_picker, outputs=pdf_file)\n",
    "\n",
    "                    gr.Markdown(\"## ‚ûï N·∫°p th√™m PDF m·ªõi\")\n",
    "                    uploader = gr.File(label=\"Ch·ªçn file PDF\", file_types=[\".pdf\"], file_count=\"multiple\")\n",
    "                    ingest_btn = gr.Button(\"N·∫°p & c·∫≠p nh·∫≠t kho\", variant=\"secondary\")\n",
    "                    ingest_status = gr.Textbox(label=\"Tr·∫°ng th√°i\", lines=4, interactive=False)\n",
    "\n",
    "                    ingest_btn.click(fn=ui_upload_and_ingest, inputs=uploader, outputs=[ingest_status, pdf_picker, pdf_file])\n",
    "\n",
    "                with gr.Tab(\"üß© Ph∆∞∆°ng ph√°p\"):\n",
    "                    gr.Markdown(METHOD_MD)\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
